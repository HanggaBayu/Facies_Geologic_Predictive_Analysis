# -*- coding: utf-8 -*-
"""H_Submission_Predictive_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CCI0E67SshFsXGoien0H_Ja8a_TuCRWD

#**Submssion Dicoding - Predictive Analysis**
#Facies Classification Using Various Machine Learning Methods

### Nama = Hangga Bayu
### Dataset bersumber dari  https://github.com/seg/2016-ml-contest

# Import and Loading Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.colors as colors
from mpl_toolkits.axes_grid1 import make_axes_locatable

data = pd.read_csv('/content/facies_data.csv')

"""# EXPLORATORY DATA ANALYST"""

data.head()

"""The seven variables are:<br>
1. __GR__: this wireline logging tools measure gamma emission from formation. Good index for shale content.<br>
2. __ILD_log10__ : this is resistivity measurment which is applicable for identification of reservoir fluid content.<br>
3. __PE__ : photoelectric effect log can be used for lithology (mineral contet of rock) identificaiton.<br>
4. __DeltaPHI__: Phi is porosity index in petrophysics. To measure porosity, there serval methods such as neutron and density. This is differences between them.<br>
5. __PNHIND__: Average of neutron and density log.<br>
6. __NM_M__ :nonmarine-marine indicator<br>
7. __RELPOS__: relative position<br>

The nine discrete facies (classes of rocks) are:
1. __(SS)__   Nonmarine sandstone
2. __(CSiS)__ Nonmarine coarse siltstone
3. __(FSiS)__ Nonmarine fine siltstone
4. __(SiSH)__ Marine siltstone and shale
5. __(MS)__   Mudstone (limestone)
6. __(WS)__   Wackestone (limestone)
7. __(D)__    Dolomite
8. __(PS)__   Packstone-grainstone (limestone)
9. __(BS)__   Phylloid-algal bafflestone (limestone)


Facies |Label| Adjacent Facies
:---: | :---: |:--:
1 |SS| 2
2 |CSiS| 1,3
3 |FSiS| 2
4 |SiSh| 5
5 |MS| 4,6
6 |WS| 5,7
7 |D| 6,8
8 |PS| 6,7,9
9 |BS| 7,8

Let's clean up this dataset.  The 'Well Name' and 'Formation' columns can be turned into a categorical data type.
"""

data['Well Name'].value_counts()

data.isna().sum()

data.info()

data.isnull().sum()

data.describe()

data.tail()

len(data)

# CHECKING COLUMN DATA TYPE
# Assuming 'df' is your DataFrame and 'column_name' is the column you want to check
column_dtype = data['Well Name'].dtype

# Check if the dtype is 'category'
is_categorical = pd.api.types.is_categorical_dtype(column_dtype)

# Print the result
print(f"The column is categorical: {is_categorical}")

data.info()

"""#### Melihat Jumlah Pengukuran pada Masing-Masing Sumur"""

Well_Labels = ['CHURCHMAN BIBLE', 'CROSS H CATTLE', 'LUKE G U', 'NEWBY', 'NOLAN',
                 'Recruit F9', 'SHANKLE','SHRIMPLIN']

Measure_count = data['Well Name'].value_counts().sort_index()
Measure_count.index = Well_Labels

fig = plt.figure()
Measure_count.plot(kind='barh', color='green', title = 'Number of Measurements at Wells from Facies Data')

"""#### Melihat Distribusi Jumlah Masing-Masing Fasies Data"""

facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00',
       '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']

facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',
                 'WS', 'D','PS', 'BS']

facies_color_map = {}
for ind, label in enumerate(facies_labels):
    facies_color_map[label] = facies_colors[ind]

fig=plt.figure()
facies_count = data['Facies'].value_counts().sort_index()
facies_count.index = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',
                 'WS', 'D','PS', 'BS']

facies_count.plot(kind='bar',color=facies_colors,
                   title='Distribution of Facies Data')

facies_color_map

"""#### Membuat Label Fasies secara string"""

def label_facies (row, labels):
  return labels[row['Facies']-1]

data.loc[:,'Facies_Labels'] = data.apply(lambda row: label_facies(row, facies_labels), axis =1)

data.head()

data['Facies_Labels'].value_counts()

"""## Checking for Outlier"""

fig, axes = plt.subplots(nrows = 2, ncols = 3, figsize=(12,6))

axes = axes.flatten()
sns.boxplot(x = data["GR"], ax=axes[0])
axes[0].set_title("Boxplot of GR")

sns.boxplot(x = data["ILD_log10"], ax=axes[1])
axes[1].set_title("Boxplot of ILD_log10")

sns.boxplot(x = data["DeltaPHI"], ax=axes[2])
axes[2].set_title("Boxplot of DeltaPHI")

sns.boxplot(x = data["PHIND"], ax=axes[3])
axes[3].set_title("Boxplot of PHIND")

sns.boxplot(x = data["PE"], ax=axes[4])
axes[4].set_title("Boxplot of PE")

sns.boxplot(x = data["RELPOS"], ax=axes[5])
axes[5].set_title("Boxplot of RELPOS")

plt.tight_layout()
plt.show()

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3-Q1

data = data [~((data< (Q1-1.5*IQR))|(data>(Q3+1.5*IQR))).any(axis=1)]

data.info()

len(data)

"""#### Mengubah Fitur-Fitur Menjadi Tipe Data Kategori"""

data['Well Name'] = data['Well Name'].astype('category')
data['Formation'] = data['Formation'].astype('category')

"""#### Melakukan Plottinng pada Salah Satu Sumur"""

def make_facies_log_plot(logs, facies_colors):
    #make sure logs are sorted by depth
    logs = logs.sort_values(by='Depth')
    cmap_facies = colors.ListedColormap(
            facies_colors[0:len(facies_colors)], 'indexed')

    ztop=logs.Depth.min(); zbot=logs.Depth.max()

    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)

    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(12, 6))
    ax[0].plot(logs.GR, logs.Depth, '-g')
    ax[1].plot(logs.ILD_log10, logs.Depth, '-')
    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.40')
    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')
    ax[4].plot(logs.PE, logs.Depth, '-', color='black')
    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',
                    cmap=cmap_facies,vmin=1,vmax=9)

    divider = make_axes_locatable(ax[5])
    cax = divider.append_axes("right", size="20%", pad=0.05)
    cbar=plt.colorbar(im, cax=cax)
    cbar.set_label((5*' ').join([' SS ', 'CSiS', 'FSiS',
                                'SiSh', ' MS ', ' WS ', ' D  ',
                                ' PS ', ' BS ']))
    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')

    for i in range(len(ax)-1):
        ax[i].set_ylim(ztop,zbot)
        ax[i].invert_yaxis()
        ax[i].grid()
        ax[i].locator_params(axis='x', nbins=3)

    ax[0].set_xlabel("GR")
    ax[0].set_xlim(logs.GR.min(),logs.GR.max())
    ax[1].set_xlabel("ILD_log10")
    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())
    ax[2].set_xlabel("DeltaPHI")
    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())
    ax[3].set_xlabel("PHIND")
    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())
    ax[4].set_xlabel("PE")
    ax[4].set_xlim(logs.PE.min(),logs.PE.max())
    ax[5].set_xlabel('Facies')

    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])
    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])
    ax[5].set_xticklabels([])
    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)

make_facies_log_plot(data[data["Well Name"]=='SHRIMPLIN'],facies_colors)

"""## Univariate Analysis"""

data.info()

numerical_features = data.select_dtypes(include=['float64','int64']).columns.tolist()
categorical_features = data.select_dtypes(include=['category']).columns.tolist()
numerical_features,categorical_features

"""### Categorical Features"""

categorical_features[0]

for i in categorical_features:
  print (i)

for feature in categorical_features:
  count = data[feature].value_counts()
  percent = 100*data[feature].value_counts(normalize=True)

  df = pd.DataFrame({'Number of Samples':count, 'Percentage':percent.round(1)})
  print('\n')
  print(f"Statistics for Feature {feature}:\n")
  print(df)
  count.plot(kind='bar', title = feature)

  plt.show()

"""### Numerical Features"""

data.hist(bins=50,figsize=(20,15))
plt.show()

"""## Multivariate Analysis
### Numerical Features
"""

data.head()

sns.set()
sns.pairplot(data.drop(['Facies','Depth','NM_M','RELPOS'],axis=1),
             hue='Facies_Labels', palette=facies_color_map,
             hue_order=list(reversed(facies_labels)))

"""## Crossplot Analysis
Crossplot dilakukan untuk melihat hubungan antara 1 variabel terhadap variabel lainnya dalam koordinat kartesius
"""

import seaborn as sns
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel('ILD_log10')

for i in (1, 2, 7):
    subset = data[data['Facies'] == i]
    x = subset['ILD_log10']
    y = subset['GR']
    label = f"Facies {i}: " + (facies_labels[i-1])
    sns.regplot(x=x, y=y, marker='+', label=label, ci=1)

ax.legend(loc='best')
plt.title('Crossplot ILD_log10 vs GR', pad = 20)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel('PHIND')

for i in (1, 3, 7):
    subset = data[data['Facies'] == i]
    x = pd.np.log10(subset['PHIND'])
    y = subset['ILD_log10']
    label = f"Facies {i}: " + (facies_labels[i-1])
    sns.regplot(x=x, y=y, marker='+', label=label, ci=1)

ax.legend(loc='best')
plt.title('Crossplot PHIND vs ILD_log10', pad = 20)
plt.show()

"""## Correlation Matrix"""

plt.figure(figsize=(10,8))
corr_matrix = data.corr().round(2)

sns.heatmap(data = corr_matrix, annot = True, cmap = 'coolwarm', linewidth = 0.4)
plt.title('Correlation Matrix for Numerical Features', size = 18, pad = 10)

"""# DATA PREPARATION

## Feature Augmentation & Splitting Data
"""

# Code borrowed from Top winner LA Team https://github.com/seg/2016-ml-contest/blob/master/LA_Team/
# Feature windows concatenation function
def augment_features_window(X, N_neig):

    # Parameters
    N_row = X.shape[0]
    N_feat = X.shape[1]

    # Zero padding
    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))

    # Loop over windows
    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))
    for r in np.arange(N_row)+N_neig:
        this_row = []
        for c in np.arange(-N_neig,N_neig+1):
            this_row = np.hstack((this_row, X[r+c]))
        X_aug[r-N_neig] = this_row

    return X_aug


# Feature gradient computation function
def augment_features_gradient(X, depth):

    # Compute features gradient
    d_diff = np.diff(depth).reshape((-1, 1))
    d_diff[d_diff==0] = 0.001
    X_diff = np.diff(X, axis=0)
    X_grad = X_diff / d_diff

    # Compensate for last missing value
    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))

    return X_grad

# Feature augmentation function
def augment_features(X, well, depth, N_neig=1):
#def augment_features(X, well, depth, N_neig=0):

    # Augment features
    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))
    for w in np.unique(well):
        w_idx = np.where(well == w)[0]
        X_aug_win = augment_features_window(X[w_idx, :], N_neig)
        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])
        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)

    # Find padded rows
    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])

    return X_aug, padded_rows

# SPLITTING DATA FOR TRAINING (TRAINING AND VALIDATION) AND TESTING
facies_train = data[data['Well Name']!='NEWBY']
facies_test = data[data['Well Name']=='NEWBY']

len(facies_train), len(facies_test), facies_train.shape, facies_test.shape

features = ['GR', 'ILD_log10', 'DeltaPHI','PHIND','PE','NM_M', 'RELPOS']
data_train = facies_train[features].values
label_train = facies_train['Facies'].values

data_test = facies_test[features].values
label_test = facies_test['Facies'].values

#Make Variable to Store Value
well_train = facies_train['Well Name'].values
depth_train = facies_train['Depth'].values

well_test = facies_test['Well Name'].values
depth_test = facies_test['Depth'].values

data_train.shape, data_test.shape

X_aug_train, padded_rows_train = augment_features(data_train, well_train, depth_train)
X_aug_test, padded_rows_test = augment_features(data_test, well_test, depth_test)

X_aug_train.shape, X_aug_test.shape

"""## Standarization"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_aug_train)
scaled_X_train = scaler.transform(X_aug_train)
scaled_X_test = scaler.transform (X_aug_test)

"""#### Splitting Data untuk digunakan dalam proses training"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(scaled_X_train,label_train, test_size = 0.1, stratify=label_train)

X_train.shape, X_test.shape

"""# MODEL DEVELOPMENT

## Grid Search and RandomSearch
Dilakukan untuk menentukan parameter terbaik dalam suatu algoritma
"""

from sklearn.model_selection import GridSearchCV,RandomizedSearchCV

from sklearn.model_selection import ShuffleSplit

from sklearn import ensemble
from sklearn.neighbors import KNeighborsClassifier
from xgboost.sklearn import XGBClassifier
from sklearn.svm import SVC

algorithm = {
      'knn':{
          'model' : KNeighborsClassifier(),
          'params':{ 'n_neighbors' : [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
                    'weights' : ['uniform', 'distance']

          }
      },
      'random_forest' :{
          'model': ensemble.RandomForestClassifier(),
          'params':{'max_depth' : [2, 3, 4],
                    'n_estimators' : [100, 500, 1000],
                    'random_state': [11, 33, 55, 77]
          }
      },
      'svm':{
          'model':SVC(),
          'params':{'C': [0.01, 1, 5, 10, 20, 50, 100, 1000, 5000, 10000],
                       'gamma' : [0.0001, 0.001, 0.01, 0.1, 1, 10]
          }
      },
      'boosting': {
            'model': ensemble.AdaBoostClassifier(),
            'params': {
                'learning_rate' : [0.1, 0.05, 0.01, 0.05, 0.001],
                'n_estimators': [25, 50, 75, 100],
                'random_state': [11, 33, 55, 77]
          }
      },
      'XGBoost':{
          'model': XGBClassifier(),
          'params':{'max_depth' : [2, 3, 4],
                    'learning_rate' : [0.001, 0.01, 0.1, 0.2, 0.4],
                    'n_estimators':[100, 500, 1000],
                    'subsample' : [0.2, 0.6, 1]
          }
      }
  }

# Create a list of dictionaries for each algorithm
data = [{'Algorithm': algo, 'Parameters': model['params']} for algo, model in algorithm.items()]

# Create a DataFrame from the list of dictionaries
df = pd.DataFrame(data)

# Display the DataFrame
print(df)

# scores = []
# cv = ShuffleSplit(n_splits=5, test_size=0.02, random_state=123)
# for algorithm_name, config in algorithm.items():
#   grid_search =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
#   grid_search.fit(X_train,y_train)
#   scores.append({
#         'model': algorithm_name,
#         'best_score': grid_search.best_score_,
#         'best_params': grid_search.best_params_
#       })

# GS_result = pd.DataFrame(scores,columns=['Model','Best_score','Best_Parameter'])

"""#### KNN Model"""

cv = ShuffleSplit(n_splits=5, test_size=0.02, random_state=123)

knn_model = algorithm['knn']['model']
knn_params = algorithm['knn']['params']
knn_search = GridSearchCV(
    knn_model,
    knn_params,
    cv=cv,
    verbose=1,
    n_jobs=-1,
)
knn_search.fit(X_train, y_train)
knn_best_params = knn_search.best_params_

print(knn_best_params)

"""#### Random Forest Model"""

rf_model = algorithm['random_forest']['model']
rf_params = algorithm['random_forest']['params']
rf_search = RandomizedSearchCV(
    rf_model,
    rf_params,
    cv=3,
    verbose=1,
    n_jobs=-1,
)
rf_search.fit(X_train, y_train)
rf_best_params = rf_search.best_params_

print(rf_best_params)

"""### SVM Model"""

svm_model = algorithm['svm']['model']
svm_params = algorithm['svm']['params']
svm_search = RandomizedSearchCV(
    svm_model,
    svm_params,
    cv = 3,
    verbose=1,
    n_jobs=-1,
)
svm_search.fit(X_train, y_train)
svm_best_params = svm_search.best_params_

print(svm_best_params)

"""#### AdaBoosting Model"""

boost_model = algorithm['boosting']['model']
boost_params = algorithm['boosting']['params']
boost_search = RandomizedSearchCV(
    boost_model,
    boost_params,
    cv=3,
    verbose=1,
    n_jobs=-1,
)
boost_search.fit(X_train, y_train)
boost_best_params = boost_search.best_params_

print(boost_best_params)

"""#### XGBoost Model"""

y_trainxg = np.array(y_train)-1
xgboost_model = algorithm['XGBoost']['model']
xgboost_params = algorithm['XGBoost']['params']
xgboost_search = RandomizedSearchCV(
    xgboost_model,
    xgboost_params,
    cv=3,
    verbose=1,
    n_jobs=-1,
)
xgboost_search.fit(X_train, y_trainxg)
xgboost_best_params = xgboost_search.best_params_

print(xgboost_best_params)

from IPython.core.display import HTML

results_data = {
    'Algorithm': ['knn', 'random_forest','svm','AdaBoost','XGBoost'],
    'Best Parameters': [knn_best_params, rf_best_params,svm_best_params,boost_best_params,xgboost_best_params]
}

# Creating DataFrame
results_df = pd.DataFrame(results_data)

pd.set_option('display.max_colwidth', None)
html_table = results_df.to_html(index=False)

# Display the HTML table horizontally
display(HTML(html_table))

"""# Training with Model with Best Parameter
Setelah diperoleh parameter terbaik pada masing-masing algoritma, maka dilakukan proses training dengan paramater tersebut
"""

# Prepare dataframe for model analysis
models = pd.DataFrame(index = ['train_acc','test_acc'],
                      columns = ['KNN','RandomForest','SVM','Boosting','XGBoost'])

models

target_names = ['SS', 'CSiS', 'FSiS', 'SiSh','MS', 'WS', 'D','PS', 'BS']

"""### KNN"""

from sklearn.metrics import accuracy_score, classification_report

knn = KNeighborsClassifier(n_neighbors = 6, weights = 'distance')
knn.fit(X_train,y_train)

models.loc['train_acc','KNN'] = accuracy_score(y_true = y_train, y_pred = knn.predict(X_train))
models.loc['test_acc','KNN'] = accuracy_score(y_true= y_test, y_pred = knn.predict(X_test))

models

print (classification_report(y_test, knn.predict(X_test), target_names=target_names))

"""### Random Forest"""

RF = ensemble.RandomForestClassifier(max_depth = 4, n_estimators = 100,random_state =77 )
RF.fit(X_train, y_train)

models.loc['train_acc','RandomForest'] = accuracy_score(y_true = y_train, y_pred = RF.predict(X_train))
models.loc['test_acc','RandomForest'] = accuracy_score(y_true= y_test, y_pred = RF.predict(X_test))

models

print (classification_report(y_test, RF.predict(X_test), target_names=target_names))

"""### SVM"""

SVM = SVC(gamma = 0.1, C=1000 )
SVM.fit(X_train, y_train)

models.loc['train_acc','SVM'] = accuracy_score(y_true = y_train, y_pred = SVM.predict(X_train))
models.loc['test_acc','SVM'] = accuracy_score(y_true= y_test, y_pred = SVM.predict(X_test))

models

print (classification_report(y_test, SVM.predict(X_test), target_names=target_names))

"""### Boosting"""

from sklearn.metrics import accuracy_score, classification_report

AdaBoost = ensemble.AdaBoostClassifier(learning_rate = 0.05, n_estimators =25,random_state=33 )
AdaBoost.fit(X_train,y_train)

models.loc['train_acc','Boosting'] = accuracy_score(y_true = y_train, y_pred = AdaBoost.predict(X_train))
models.loc['test_acc','Boosting'] = accuracy_score(y_true= y_test, y_pred = AdaBoost.predict(X_test))

models

print (classification_report(y_test, AdaBoost.predict(X_test), target_names=target_names))

"""### XGBoost"""

y_trainxg = np.array(y_train)-1
X_trainxg = np.array(X_train)-1
y_testxg = np.array(y_test)-1
X_testxg = np.array(X_test)-1
XGBoost = XGBClassifier(sub_sample =1 , max_depth = 3,learning_rate = 0.4, n_estimators =1000 )
XGBoost.fit(X_trainxg,y_trainxg)

models.loc['train_acc','XGBoost'] = accuracy_score(y_true = y_trainxg, y_pred = XGBoost.predict(X_trainxg))
models.loc['test_acc','XGBoost'] = accuracy_score(y_true= y_testxg, y_pred = XGBoost.predict(X_testxg))

models

print(classification_report(y_testxg,XGBoost.predict(X_testxg),target_names=target_names, digits=3))

"""# Evaluation"""

# Menampilkan perbandingan akurasi beberapa model yang telah dibuat
plt.bar('KNN', models.loc['test_acc','KNN'])
plt.bar('RandomForest', models.loc['test_acc','RandomForest'])
plt.bar('SVM', models.loc['test_acc','SVM'])
plt.bar('AdaBoost', models.loc['test_acc','Boosting'])
plt.bar('XGBoost', models.loc['test_acc','XGBoost'])
plt.title("Perbandingan Akurasi Model");
plt.xlabel('Model');
plt.ylabel('Akurasi');
plt.show()

"""Setelah dilakukan proses training dan dilakukan pengukuran accuracy pada data validasi, diperoleh XG Boost, SVM, dan KNN merupakan model terbaik secara berurutan

# Model Prediction Using Data Test

#### Data test yang digunakan adalah data pada sumur NEWBY
"""

facies_test

"""#### Membuat plot untuk membandingkan hasil prediksi fasies batuan dan label aslinya"""

def compare_facies_plot(logs, compadre, facies_colors):
    #make sure logs are sorted by depth
    logs = logs.sort_values(by='Depth')
    cmap_facies = colors.ListedColormap(
            facies_colors[0:len(facies_colors)], 'indexed')

    ztop=logs.Depth.min(); zbot=logs.Depth.max()

    cluster1 = np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)
    cluster2 = np.repeat(np.expand_dims(logs[compadre].values,1), 100, 1)

    f, ax = plt.subplots(nrows=1, ncols=7, figsize=(9, 12))
    ax[0].plot(logs.GR, logs.Depth, '-g')
    ax[1].plot(logs.ILD_log10, logs.Depth, '-')
    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')
    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')
    ax[4].plot(logs.PE, logs.Depth, '-', color='black')
    im1 = ax[5].imshow(cluster1, interpolation='none', aspect='auto',
                    cmap=cmap_facies,vmin=1,vmax=9)
    im2 = ax[6].imshow(cluster2, interpolation='none', aspect='auto',
                    cmap=cmap_facies,vmin=1,vmax=9)

    divider = make_axes_locatable(ax[6])
    cax = divider.append_axes("right", size="20%", pad=0.05)
    cbar=plt.colorbar(im2, cax=cax)
    cbar.set_label((17*' ').join([' SS ', 'CSiS', 'FSiS',
                                'SiSh', ' MS ', ' WS ', ' D  ',
                                ' PS ', ' BS ']))
    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')

    for i in range(len(ax)-2):
        ax[i].set_ylim(ztop,zbot)
        ax[i].invert_yaxis()
        ax[i].grid()
        ax[i].locator_params(axis='x', nbins=3)

    ax[0].set_xlabel("GR")
    ax[0].set_xlim(logs.GR.min(),logs.GR.max())
    ax[1].set_xlabel("ILD_log10")
    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())
    ax[2].set_xlabel("DeltaPHI")
    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())
    ax[3].set_xlabel("PHIND")
    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())
    ax[4].set_xlabel("PE")
    ax[4].set_xlim(logs.PE.min(),logs.PE.max())
    ax[5].set_xlabel('Facies')
    ax[6].set_xlabel(compadre)

    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])
    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])
    ax[5].set_xticklabels([])
    ax[6].set_xticklabels([])
    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)

"""#### Prediksi dengan SVM"""

y_pred2 = SVM.predict(scaled_X_test)

facies_test ['pred_SVM'] = y_pred2

compare_facies_plot(facies_test.sort_values(by='Depth'), 'pred_SVM', facies_colors)

"""#### Prediksi dengan KNN"""

y_pred = knn.predict(scaled_X_test)

facies_test ['pred_KNN'] = y_pred

compare_facies_plot(facies_test.sort_values(by='Depth'), 'pred_KNN', facies_colors)

"""### Prediksi dengan XGBoost"""

X_predxg = np.array(scaled_X_test)-1

y_predxg = XGBoost.predict(X_predxg )

facies_test ['pred_XGBoost'] = y_predxg+1

compare_facies_plot(facies_test.sort_values(by='Depth'), 'pred_XGBoost', facies_colors)